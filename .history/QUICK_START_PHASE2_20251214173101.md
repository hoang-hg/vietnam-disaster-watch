# ğŸš€ Quick Start - Phase 2 Tools

## âš¡ One-Command Setup

```bash
cd backend

# 1ï¸âƒ£ Validate RSS sources
python test_rss_sources.py

# 2ï¸âƒ£ Run single crawl with dedup
python -m app.crawler --once

# 3ï¸âƒ£ Check source health (last 24 hours)
python monitor_rss_health.py 1

# 4ï¸âƒ£ Analyze NLP quality
python analyze_nlp_quality.py
```

## ğŸ“Š What Each Tool Does

| Tool                     | Purpose                    | Command                               |
| ------------------------ | -------------------------- | ------------------------------------- |
| `test_rss_sources.py`    | Validate RSS URLs          | `python test_rss_sources.py`          |
| `crawler.py`             | Crawl + dedup + fallback   | `python -m app.crawler --once`        |
| `monitor_rss_health.py`  | Track source health        | `python monitor_rss_health.py [days]` |
| `analyze_nlp_quality.py` | Compare extraction quality | `python analyze_nlp_quality.py`       |
| `test_crawl_cycles.py`   | Load test                  | `python test_crawl_cycles.py [runs]`  |

## ğŸ¯ Key Metrics to Monitor

### Health Dashboard

```
âœ… Working: Sources with 100% success
ğŸŸ¢ Healthy: 75-100% success
ğŸŸ¡ Warning: 50-75% success
ğŸ”´ Critical: <50% success
```

### NLP Quality

- **Official RSS:** 6.7% articles with impact data
- **GNews:** 0.0% articles with impact data
- **Goal:** Increase official RSS coverage to improve accuracy

### Dedup Stats

- Same-source duplicates: SKIPPED âœ…
- Cross-source duplicates: Caught with 75% title similarity
- Log output: `[DEDUP] {source} #{hash}: duplicate of {original} (skipped)`

## ğŸ”§ Troubleshooting

### No log data?

```bash
# Logs stored here:
ls backend/logs/crawl_log.jsonl

# Run crawl first:
python -m app.crawler --once
```

### Sources not working?

```bash
# Check RSS URLs:
python test_rss_sources.py

# Update sources.json if URLs changed
# Format: { "name", "domain", "primary_rss", "backup_rss" }
```

### High dedup rate?

- Normal if same sources report same events
- Check if cross-domain dedup is working correctly
- Adjust similarity threshold in `dedup.py` if needed

## ğŸ“ˆ Expected Performance

| Metric                         | Value                                |
| ------------------------------ | ------------------------------------ |
| Articles/crawl                 | 60-104                               |
| RSS success rate               | 16.7% (2/12)                         |
| Dedup effectiveness            | Medium (same-source dups eliminated) |
| NLP improvement (RSS vs GNews) | +6.7% impact data                    |
| Crawl time                     | ~12 seconds                          |

## ğŸ“ Architecture Overview

```
Crawler â†’ Fetch RSS/GNews â†’ Parse â†’ Dedup Check â†’ Extract NLP â†’ Insert DB
                                         â†“
                                   [Skip if duplicate]

Health Monitor â†’ Parse crawl_log.jsonl â†’ Per-source stats â†’ Report
Quality Tool â†’ Analyze DB â†’ Impact extraction rates â†’ Comparison
```

## âœ… Phase 2 Verification Checklist

- [x] RSS-first + fallback strategy working
- [x] Deduplication preventing same-source duplicates
- [x] Health monitoring tracking source reliability
- [x] NLP quality shows improvement with official RSS
- [x] Centralized source config (sources.json)
- [x] Comprehensive logging and reporting

---

**Next:** Consider Phase 3 for HTML scraper + expanded RSS coverage
